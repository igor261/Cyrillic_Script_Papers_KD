{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# read csv\n",
    "import pandas as pd\n",
    "import os\n",
    "# PdfMiner\n",
    "import glob\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "# Token Vectorization\n",
    "from langdetect import detect \n",
    "import fasttext.util\n",
    "import fasttext\n",
    "########################################\n",
    "from random import random\n",
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "########################################\n",
    "import regex as re  \n",
    "import string\n",
    "import re\n",
    "########################################\n",
    "from datetime import datetime\n",
    "import collections\n",
    "########################################\n",
    "# Keras imports for ML Model\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, LSTM, Bidirectional\n",
    "from keras.layers import Dropout, Flatten, GlobalAveragePooling2D, Embedding\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Meine Dateien\\\\Kit Wiing Master\\\\SS 20\\\\Seminar - Knowledge Discovery\\\\Cyrillic_Script_Papers_KD'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "# CHANGE PATH!\n",
    "model = tf.keras.models.load_model(\"D:/_Training_on_all/Adam-Epochs-25-dropout-0-4-dim-100-Recall.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 1000, 200)         164800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000, 100)         20100     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1000, 100)         0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000, 4)           404       \n",
      "=================================================================\n",
      "Total params: 185,304\n",
      "Trainable params: 185,304\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "### load FastText Modules\n",
    "# CHANGE PATH!\n",
    "# path_to_bins ='C:/_KDDM_Seminar/python/fastText/_bins/'\n",
    "path_to_bins ='D:/fastText/' # Igor\n",
    "### import RU fasttext model in reduced version (100 dim.)\n",
    "ft_ru = fasttext.load_model(path_to_bins+'cc.ru.100.bin')\n",
    "### import UK fasttext model in reduced version (100 dim.)\n",
    "ft_uk = fasttext.load_model(path_to_bins+'cc.uk.100.bin')\n",
    "### import BG fasttext model in reduced version (100 dim.)\n",
    "ft_bg = fasttext.load_model(path_to_bins+'cc.bg.100.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Funktion zum entfernen von Zeilenumbrüchen\n",
    "def removePassage(my_str):\n",
    "    my_str1 = re.sub(\"\\\\\\\\ud\", \" \", my_str)\n",
    "    my_str2 = re.sub(\"\\\\\\\\n\", \" \", my_str1)\n",
    "    return(my_str2)\n",
    "\n",
    "### Funktion zum parsen von PDF[nur erste Seite] zu String Format\n",
    "def extract_page_one(path):\n",
    "    output_string = StringIO()\n",
    "    \n",
    "    with open(path, 'rb') as in_file:\n",
    "        parser = PDFParser(in_file)\n",
    "        doc = PDFDocument(parser)\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        interpreter.process_page(list(PDFPage.create_pages(doc))[0])\n",
    "        return(output_string)\n",
    "    \n",
    "### Funktion zum entfernen von 'Arrays' aus Autoren\n",
    "def removeAutor(my_str):\n",
    "    my_str1 = re.sub(\"\\['\", \"\", my_str)\n",
    "    my_str2 = re.sub(\"'\\]\", \"\", my_str1)\n",
    "    my_str3 = re.sub(\"'\", \"\", my_str2)\n",
    "    return(my_str3)\n",
    "\n",
    "### Funktion zum Vergrößern der Input & Output Vektoren um NEWLINES\n",
    "def add_newlines(Tokens,Real_Tokens,y_final):\n",
    "    y_final_REAL = []\n",
    "    k = 0\n",
    "    m = 0\n",
    "    for i in range(len(Tokens)):\n",
    "        if k == 0:\n",
    "            j=i\n",
    "        else:\n",
    "            if m == 0:\n",
    "                j = k+1\n",
    "            else:\n",
    "                j = m+1\n",
    "        if Tokens[i] == Real_Tokens[j] : # Wenn Tokens gelich sind, dann übernehme y_final_REAL\n",
    "            y_final_REAL.append(y_final[i])\n",
    "            m = j\n",
    "        else:\n",
    "            for k in range(j,len(Real_Tokens)): # Sonst gehe die Real_Tokens durch bis match vorhanden\n",
    "\n",
    "                if Real_Tokens[k] == 'NEWLINE':\n",
    "                    y_final_REAL.append('Sonstiges')\n",
    "                \n",
    "#                 elif Real_Tokens[k] not in Tokens:\n",
    "#                     y_final_REAL.append('Sonstiges')\n",
    "\n",
    "                else:\n",
    "                    y_final_REAL.append(y_final[i])\n",
    "                    m=k\n",
    "                    break\n",
    "\n",
    "    RealTokens_final = Real_Tokens[:len(y_final_REAL)]\n",
    "    \n",
    "    index_title = [i for i, e in enumerate(y_final_REAL) if e == 'I-title']\n",
    "    end_title = max(index_title)\n",
    "    \n",
    "    ### lable NEWLINES im Titel als \"I-title\"        \n",
    "    for i in range(len(RealTokens_final)):\n",
    "        if RealTokens_final[i]=='NEWLINE':\n",
    "            if (y_final_REAL[i+1] =='I-title' or end_title>i) and y_final_REAL[i-1] in ('B-title','I-title'):\n",
    "                y_final_REAL[i] = 'I-title'\n",
    "    ## Kann passieren, dass im Titel mehrere NEWLINES aufeinander folgen. Daher end_title>i um festzustellen, ob iwann später noch ein Titel Label kommt        \n",
    "                \n",
    "    return(RealTokens_final,y_final_REAL)\n",
    "\n",
    "#Create Word Vector representation\n",
    "def detect_and_vectorize(tokens_sequence): #### input is the tokens list of ONE PAPER e.g. ergebnis_tokens[2]\n",
    "    \n",
    "    tokens_vectorized = []\n",
    "    lang = detect(' '.join(tokens_sequence))\n",
    "    \n",
    "    if (lang == 'ru'):\n",
    "        for i in range(len(tokens_sequence)):\n",
    "            tokens_vectorized.append(np.float16(ft_ru.get_word_vector(tokens_sequence[i])))\n",
    "                \n",
    "    elif (lang == 'bg'):\n",
    "        for i in range(len(tokens_sequence)):\n",
    "            tokens_vectorized.append(np.float16(ft_bg.get_word_vector(tokens_sequence[i])))\n",
    "                \n",
    "    else:  ## assume language == uk\n",
    "        for i in range(len(tokens_sequence)):\n",
    "            tokens_vectorized.append(np.float16(ft_uk.get_word_vector(tokens_sequence[i])))\n",
    "    \n",
    "    while len(tokens_vectorized)<1000:\n",
    "        tokens_vectorized.append(np.zeros(100))\n",
    "      \n",
    "    if len(tokens_vectorized)>1000:\n",
    "        del tokens_vectorized[1000:] \n",
    "\n",
    "    return np.array(tokens_vectorized)\n",
    "\n",
    "### Additional Features\n",
    "punctuations = '''!()[]{};:'\"\\<>/?@#$%^&*«»_–~.,-'''\n",
    "\n",
    "def compute_additional_features(tokens_sequence): #### input is the tokens list of ONE PAPER e.g. ergebnis_tokens[2]\n",
    "    \n",
    "    tokens = tokens_sequence\n",
    "    feature_upper = []\n",
    "    feature_capitalized = []\n",
    "    feature_autor_format = []\n",
    "    feature_punctation = []\n",
    "    feature_newline = []\n",
    "    feature_array = []\n",
    "    \n",
    "    while len(tokens)<1000:\n",
    "        tokens.append(str(0))\n",
    "    if len(tokens)>1000:\n",
    "        del tokens[1000:] \n",
    "    #print(tokens)    \n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] !='NEWLINE':\n",
    "            if str(tokens[i]).isupper():\n",
    "                feature_upper.append(1)\n",
    "\n",
    "            else:\n",
    "                feature_upper.append(0)\n",
    "        else: \n",
    "            feature_upper.append(0)\n",
    "\n",
    "        if tokens[i] !='NEWLINE':\n",
    "            if str(tokens[i][0]).isupper():\n",
    "                feature_capitalized.append(1)\n",
    "\n",
    "            else:\n",
    "                feature_capitalized.append(0)\n",
    "        else: \n",
    "            feature_capitalized.append(0)\n",
    "\n",
    "        if tokens[i] !='NEWLINE':\n",
    "            if re.match('.\\.',str(tokens[i])) != None and str(tokens[i]).isupper():\n",
    "                feature_autor_format.append(1)\n",
    "\n",
    "            else:\n",
    "                feature_autor_format.append(0)\n",
    "        else: \n",
    "            feature_autor_format.append(0)\n",
    "\n",
    "        if tokens[i] !='NEWLINE':\n",
    "            if any((c in punctuations) for c in str(tokens[i])):\n",
    "                feature_punctation.append(1)\n",
    "            else:\n",
    "                feature_punctation.append(0)\n",
    "        else: \n",
    "            feature_punctation.append(0)\n",
    "                \n",
    "        if tokens[i] =='NEWLINE':\n",
    "            feature_newline.append(1)\n",
    "        else: \n",
    "            feature_newline.append(0)\n",
    "    df = pd.DataFrame(list(zip(feature_upper, feature_capitalized,feature_autor_format ,feature_punctation,feature_newline)))  \n",
    "    feature_array = df.to_numpy(copy=True)\n",
    "    \n",
    "    return np.array(feature_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE PATH!\n",
    "path_meta = 'D:/_final_selection_16478/'\n",
    "path_papers = \"D:/_final_selection_16478/PDFs_15553\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coreId</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11083759</td>\n",
       "      <td>Середовище проведення освітніх вебінарів «WIP ...</td>\n",
       "      <td>['Богачков, Ю.М.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11083794</td>\n",
       "      <td>Комплексна підготовка дистанційних матеріалів</td>\n",
       "      <td>['Богачков, Ю.М.', 'Царенко, В.О.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11083801</td>\n",
       "      <td>Метод аналізу елементарних доменів для покраще...</td>\n",
       "      <td>['Богачков, Ю.М.', 'Ухань, П.С.', 'Полобюк, Т....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11083807</td>\n",
       "      <td>Інструментальні засоби профорієнтації</td>\n",
       "      <td>['Богачков, Ю.М.', 'Буров, О.Ю.', 'Бельська, Н...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11083814</td>\n",
       "      <td>Експеримент «Дистанційне навчання школярів”: п...</td>\n",
       "      <td>['Богачков, Ю.М.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15548</th>\n",
       "      <td>95313001</td>\n",
       "      <td>Підігрів тротуарів за допомогою теплових мереж</td>\n",
       "      <td>['Семенюк, К.А.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15549</th>\n",
       "      <td>95313006</td>\n",
       "      <td>Шлакозоловидалення як спосіб підвищення енерго...</td>\n",
       "      <td>['Бондар, Г.Ю.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15550</th>\n",
       "      <td>95313010</td>\n",
       "      <td>Підвищення ефективності роботи каміну</td>\n",
       "      <td>['Півень, Д.В.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15551</th>\n",
       "      <td>95313012</td>\n",
       "      <td>Технологія поквартирного обліку теплової енергії</td>\n",
       "      <td>['Горбачьова, А.О.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15552</th>\n",
       "      <td>95313017</td>\n",
       "      <td>Використання нового покоління будинкових регул...</td>\n",
       "      <td>['Назаренко, Д.В.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15553 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         coreId                                              title  \\\n",
       "0      11083759  Середовище проведення освітніх вебінарів «WIP ...   \n",
       "1      11083794      Комплексна підготовка дистанційних матеріалів   \n",
       "2      11083801  Метод аналізу елементарних доменів для покраще...   \n",
       "3      11083807              Інструментальні засоби профорієнтації   \n",
       "4      11083814  Експеримент «Дистанційне навчання школярів”: п...   \n",
       "...         ...                                                ...   \n",
       "15548  95313001     Підігрів тротуарів за допомогою теплових мереж   \n",
       "15549  95313006  Шлакозоловидалення як спосіб підвищення енерго...   \n",
       "15550  95313010              Підвищення ефективності роботи каміну   \n",
       "15551  95313012   Технологія поквартирного обліку теплової енергії   \n",
       "15552  95313017  Використання нового покоління будинкових регул...   \n",
       "\n",
       "                                                 authors  \n",
       "0                                     ['Богачков, Ю.М.']  \n",
       "1                    ['Богачков, Ю.М.', 'Царенко, В.О.']  \n",
       "2      ['Богачков, Ю.М.', 'Ухань, П.С.', 'Полобюк, Т....  \n",
       "3      ['Богачков, Ю.М.', 'Буров, О.Ю.', 'Бельська, Н...  \n",
       "4                                     ['Богачков, Ю.М.']  \n",
       "...                                                  ...  \n",
       "15548                                  ['Семенюк, К.А.']  \n",
       "15549                                   ['Бондар, Г.Ю.']  \n",
       "15550                                   ['Півень, Д.В.']  \n",
       "15551                               ['Горбачьова, А.О.']  \n",
       "15552                                ['Назаренко, Д.В.']  \n",
       "\n",
       "[15553 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Import Meta Data\n",
    "\n",
    "df_meta = pd.read_csv(path_meta + 'final_items_15553.csv', sep = ',')#  , encoding= 'utf-16')\n",
    "# df_meta.drop('Unnamed: 0' , axis = 1 , inplace=True)\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3067</th>\n",
       "      <td>14181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3068</th>\n",
       "      <td>8077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069</th>\n",
       "      <td>3046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3070</th>\n",
       "      <td>8585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3071</th>\n",
       "      <td>7115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3072 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0      9648\n",
       "1      4767\n",
       "2      2852\n",
       "3      9840\n",
       "4     14077\n",
       "...     ...\n",
       "3067  14181\n",
       "3068   8077\n",
       "3069   3046\n",
       "3070   8585\n",
       "3071   7115\n",
       "\n",
       "[3072 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Import Indices of the Test set\n",
    "# CHANGE PATH!\n",
    "df_test_id = pd.read_csv(\"D:\\_Training_on_all\\Test_Indices_Adam-Epochs-25-dropout-0-4-dim-100.csv\", sep = ',',header=None)#  , encoding= 'utf-16')\n",
    "df_test_id = df_test_id.T\n",
    "df_test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Core_ID_12086585.pdf',\n",
       " 'Core_ID_11336429.pdf',\n",
       " 'Core_ID_11328295.pdf',\n",
       " 'Core_ID_12086885.pdf',\n",
       " 'Core_ID_42032087.pdf']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_15553 = os.listdir(path_papers)\n",
    "\n",
    "all_pdfs = []\n",
    "for elem in df_test_id[0]:\n",
    "    all_pdfs.append(pdf_15553[elem])\n",
    "    \n",
    "all_pdfs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_coreId = []\n",
    "files_paths = []\n",
    "for i in all_pdfs:\n",
    "    files_paths.append(path_papers +\"\\\\\" + i)\n",
    "    files_coreId.append(re.sub(\".pdf\",\"\",re.sub(\"Core_ID_\",\"\",i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### extract text \n",
    "\n",
    "all_pdf_text = [] \n",
    "start_time = datetime.now()\n",
    "for i in range(len(files_paths)):\n",
    "\n",
    "    all_pdf_text.append(extract_page_one(files_paths[i]).getvalue())\n",
    "#     print(str((i/len(files_paths))*100)+'%')\n",
    "    \n",
    "    \n",
    "end_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time duration for text extractions: 9.377626516666668 min.\n"
     ]
    }
   ],
   "source": [
    "time_delta = (end_time - start_time)\n",
    "seconds_all = time_delta.total_seconds()\n",
    "minutes_all = seconds_all/60\n",
    "print('Time duration for text extractions: '+str(minutes_all)+' min.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define files_core_id\n",
    "\n",
    "files_core_id = files_coreId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get all titles from meta data with core_ids of fulltext\n",
    "titles = []\n",
    "for i in range(len(files_core_id)):\n",
    "    index = df_meta.index[df_meta['coreId'] == int(files_core_id[i])].tolist()\n",
    "    if index == []:\n",
    "        titles.append('Keine Meta Daten gefunden')\n",
    "    else: \n",
    "        index = index[0]\n",
    "        title_pdf  = df_meta.loc[index,'title']\n",
    "        titles.append(title_pdf)\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get autor for the PDF´s\n",
    "######## PROBLEM: verschiedene Schreibweisen Meta <-> PDF\n",
    "autors = []\n",
    "for i in range(len(files_core_id)):\n",
    "    index = df_meta.index[df_meta['coreId'] == int(files_core_id[i])].tolist()\n",
    "    index = index[0]\n",
    "    autor_pdf  = df_meta.loc[index,'authors']\n",
    "\n",
    "    autor_pdf = removeAutor(autor_pdf).split(\",\")\n",
    "    for j in range(len(autor_pdf)):\n",
    "        autor_pdf[j] = ' '.join(autor_pdf[j].split()) ## Entferne überflüssige Whitespaces (auch am Anfang)\n",
    "        \n",
    "    autors.append(autor_pdf)\n",
    "len(autors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LABELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Funktion, die die Daten labelt \n",
    "\n",
    "kein_autor = []\n",
    "kein_titel = []\n",
    "error_papers = []\n",
    "ergebnis_tokens = []\n",
    "ergebnis_label = []\n",
    "anzahl_papers = len(files_core_id)\n",
    "\n",
    "for paper in range(anzahl_papers):\n",
    "    title = ' '.join(removePassage(titles[paper]).split()).lower() # Remove excces Whitespace & to lowercase\n",
    "    title = re.sub(\"\\(\",\"\\(\",title) # () as non-regex string\n",
    "    title = re.sub(\"\\)\",\"\\)\",title)\n",
    "\n",
    "    title_index = re.search(title, ' '.join(all_pdf_text[paper].split()).lower())\n",
    "    #print('TITEL:  ' + title)\n",
    "    \n",
    "    if title_index==None:\n",
    "        kein_titel.append(files_core_id[paper])\n",
    "    else:\n",
    "        try:\n",
    "            Text_pdf_0 = ' '.join(all_pdf_text[paper].split())\n",
    "            if title_index.start()==0:\n",
    "                teil_B = \"\"\n",
    "            else:\n",
    "                teil_B = Text_pdf_0[0:title_index.start()-1]\n",
    "            teil_T = Text_pdf_0[title_index.start():title_index.end()]\n",
    "            teil_E = Text_pdf_0[title_index.end()+1:len(Text_pdf_0)]\n",
    "\n",
    "            y_teil1 = np.repeat('Sonstiges',len(teil_B.split()))\n",
    "            y_teil2 = np.append(['B-title'],np.repeat('I-title',len(teil_T.split())-1))\n",
    "            y_teil3 = np.repeat('Sonstiges',len(teil_E.split()))\n",
    "\n",
    "            y_final = np.concatenate((y_teil1, y_teil2 , y_teil3), axis=None)\n",
    "\n",
    "            ##### Get Text\n",
    "            all_pdf_text1 = re.sub(\"\\\\n\",\" NEWLINE \",all_pdf_text[paper])\n",
    "            Text_pdf_0_NL = ' '.join(all_pdf_text1.split())\n",
    "\n",
    "            Tokens = Text_pdf_0.split()\n",
    "            Labels = y_final\n",
    "            Real_Tokens = Text_pdf_0_NL.split()\n",
    "\n",
    "            autors_surname = []\n",
    "            for i in range(len(autors[paper])):\n",
    "                if i % 2 == 0:\n",
    "                    autors_surname.append(autors[paper][i])\n",
    "\n",
    "            autors_surname_lower = []\n",
    "            for i in range(len(autors_surname)):\n",
    "                autors_surname_lower.append(autors_surname[i].lower())\n",
    "        \n",
    "            if re.match('.\\.',autors[paper][1]) == None:\n",
    "                autors_forename = []\n",
    "                for i in range(len(autors[paper])):\n",
    "                    if i % 2 == 1:\n",
    "                        autors_forename.append(autors[paper][i].split())\n",
    "\n",
    "                autors_forename = list(np.concatenate((autors_forename), axis=None))\n",
    "                autors_forename_lower = []\n",
    "                for i in range(len(autors_forename)):\n",
    "                    autors_forename_lower.append(autors_forename[i].lower())\n",
    "\n",
    "                autors_surname_lower = list(np.concatenate((autors_forename_lower,autors_surname_lower), axis=None))\n",
    "    \n",
    "            Tokens = all_pdf_text[paper].split()\n",
    "            Tokens_final_lower = []\n",
    "            for i in range(len(Tokens)):\n",
    "                Tokens_final_lower.append(Tokens[i].lower())\n",
    "\n",
    "            vec_autor = []\n",
    "            for token in Tokens_final_lower:\n",
    "                line = any(word in token for word in autors_surname_lower)\n",
    "                vec_autor.append(line)\n",
    "\n",
    "            index_autor = [i for i, e in enumerate(vec_autor) if e == True]\n",
    "\n",
    "            if len(index_autor)>(len(autors_surname_lower)):\n",
    "                diff = len(index_autor) - len(autors_surname_lower)\n",
    "                dist = []\n",
    "                for j in range(len(index_autor)):\n",
    "                    dist.append(abs(index_autor[j]-np.where(y_final==\"B-title\")[0][0]))\n",
    "\n",
    "                dict1 = dict(zip(dist , index_autor))\n",
    "\n",
    "                dist.sort(reverse = True)\n",
    "\n",
    "                for k in range(len(dist[0:diff])):\n",
    "                    vec_autor[dict1[dist[0:diff][k]]] = False\n",
    "\n",
    "            for i in range(len(y_final)):\n",
    "                if vec_autor[i] == True:\n",
    "                    y_final[i] = 'autor'\n",
    "\n",
    "            if True not in vec_autor:\n",
    "                kein_autor.append(files_core_id[paper])\n",
    "                \n",
    "            if re.match('.\\.',autors[paper][1]) != None:\n",
    "\n",
    "                index_autor_true = [i for i, e in enumerate(vec_autor) if e == True]\n",
    "\n",
    "                for w in range(len(index_autor_true)):\n",
    "                    index = index_autor_true[w]\n",
    "                    for t in range(index - 4,index + 4):\n",
    "                        if re.match('.\\.',Tokens_final_lower[t]) != None and Tokens[t].isupper():\n",
    "                            y_final[t] = 'autor'\n",
    "\n",
    "            RealTokens_final = add_newlines(Tokens,Real_Tokens,y_final)[0]\n",
    "            y_final_REAL = add_newlines(Tokens,Real_Tokens,y_final)[1]\n",
    "\n",
    "            ergebnis_label.append(y_final_REAL)\n",
    "            ergebnis_tokens.append(RealTokens_final)\n",
    "        except:\n",
    "            error_papers.append(files_core_id[paper])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kein_autor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kein_titel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for ML Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.276041666666664%\n",
      "32.55208333333333%\n",
      "48.828125%\n",
      "65.10416666666666%\n",
      "81.38020833333334%\n",
      "97.65625%\n",
      "Time duration for text vectorization: 1.2430744333333335 min.\n"
     ]
    }
   ],
   "source": [
    "### get vector representation for all PDF content and cast to float 16-bit for better RAM efficiency\n",
    "\n",
    "all_pdfs_vectorized= []\n",
    "lauf=0\n",
    "start_time = datetime.now()\n",
    "\n",
    "for i in range(len(ergebnis_tokens)):\n",
    "    all_pdfs_vectorized.append(np.float16(detect_and_vectorize(ergebnis_tokens[i])))\n",
    "#     all_pdfs_vectorized.append(detect_and_vectorize(ergebnis_tokens[i]))\n",
    "    lauf+=1\n",
    "    if lauf%500==0:\n",
    "        print(str((lauf/len(ergebnis_tokens))*100)+'%')\n",
    "    \n",
    "end_time = datetime.now()\n",
    "time_delta = (end_time - start_time)\n",
    "seconds_all = time_delta.total_seconds()\n",
    "minutes_all = seconds_all/60\n",
    "print('Time duration for text vectorization: '+str(minutes_all)+' min.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RU Replacement Count: 59979\n",
      "BG Replacement Count: 222\n"
     ]
    }
   ],
   "source": [
    "### replace newline vectors with a uniform representation (not 3 diff. vectors for bg, uk and ru)\n",
    "\n",
    "newline_vec_uk = np.float16(ft_uk.get_word_vector('NEWLINE'))\n",
    "newline_ru = np.float16(ft_ru.get_word_vector('NEWLINE'))\n",
    "newline_bg = np.float16(ft_bg.get_word_vector('NEWLINE'))\n",
    "counter_replaced_ru = 0\n",
    "counter_replaced_bg = 0\n",
    "\n",
    "for i in range(len(all_pdfs_vectorized)):\n",
    "    \n",
    "    for j in range(len(all_pdfs_vectorized[i])):\n",
    "        \n",
    "        compare_array=all_pdfs_vectorized[i][j].copy()\n",
    "        \n",
    "        if (compare_array == newline_ru).all():\n",
    "            all_pdfs_vectorized[i][j] = newline_vec_uk\n",
    "            counter_replaced_ru +=1\n",
    "            \n",
    "        elif (compare_array == newline_bg).all():\n",
    "            all_pdfs_vectorized[i][j] = newline_vec_uk\n",
    "            counter_replaced_bg +=1\n",
    "            \n",
    "print('RU Replacement Count: '+str(counter_replaced_ru))\n",
    "print('BG Replacement Count: '+str(counter_replaced_bg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072, 1000, 105)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### add feature vectors to word vectors\n",
    "\n",
    "all_pdfs_vectorized_features = []\n",
    "\n",
    "for i in range(len(all_pdfs_vectorized)):\n",
    "    pdf_vectorized_features = []\n",
    "    features_in_pdf = compute_additional_features(ergebnis_tokens[i])\n",
    "    \n",
    "    for j in range(len(all_pdfs_vectorized[i])):\n",
    "        pdf_vectorized_features.append(np.append(np.float16(all_pdfs_vectorized[i][j]),np.float16(features_in_pdf[j])))\n",
    "        \n",
    "    all_pdfs_vectorized_features.append(np.array(pdf_vectorized_features))\n",
    "\n",
    "all_pdfs_vectorized_features=np.array((all_pdfs_vectorized_features))\n",
    "all_pdfs_vectorized_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 105)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdfs_vectorized_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072, 1000, 4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### fit labels to the same length as the 1000 character input PDF string\n",
    "\n",
    "labels_categorized = []\n",
    "label_dict = {'Sonstiges':0,'B-title':1,'I-title':2,'autor':3} ## before 0 instead of None\n",
    "\n",
    "for i in range(len(ergebnis_label)):\n",
    "    transformed = [label_dict.get(n, n) for n in ergebnis_label[i]]\n",
    "    \n",
    "    while len(transformed)<1000:\n",
    "        transformed.append(np.zeros(1))\n",
    "    \n",
    "    if len(transformed)>1000:\n",
    "        del transformed[1000:] \n",
    "    \n",
    "    categorical = np_utils.to_categorical(transformed)\n",
    "    labels_categorized.append(np.array(categorical))\n",
    "\n",
    "labels_categorized = np.array(labels_categorized)\n",
    "labels_categorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = files_core_id\n",
    "\n",
    "predictions_all = []\n",
    "for i in range(len(test_files)):\n",
    "    test_exemplar_index=i\n",
    "    prediction = model.predict(all_pdfs_vectorized_features[test_exemplar_index:test_exemplar_index+1])\n",
    "    predictions_all.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all = []\n",
    "for j in range(len(test_files)):\n",
    "    results=[]\n",
    "    for i in range(1000):\n",
    "        results.append(np.argmax(predictions_all[j][0][i]))\n",
    "    results_all.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "real_all = []\n",
    "for j in range(len(test_files)):\n",
    "    real = []\n",
    "    test_exemplar_index=j\n",
    "    for i in range(1000):\n",
    "        real.append(np.argmax(labels_categorized[test_exemplar_index][i]))\n",
    "    real_all.append(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072\n",
      "3072\n"
     ]
    }
   ],
   "source": [
    "print(len(results_all))\n",
    "\n",
    "print(len(real_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Model hat eine Accuracy von: 99.22672526041666 %\n",
      "Komplett richtig klassifizierte papers: 7.421875 %\n"
     ]
    }
   ],
   "source": [
    "# Model 1\n",
    "\n",
    "acc = []\n",
    "acc_100 = []\n",
    "for i in range(len(test_files)):\n",
    "    acc.append(accuracy_score(real_all[i], results_all[i]))\n",
    "    if real_all[i]== results_all[i]:\n",
    "        acc_100.append(1)\n",
    "        \n",
    "print(\"Das Model hat eine Accuracy von: \" + str(np.mean(acc)*100)+ \" %\")\n",
    "print(\"Komplett richtig klassifizierte papers: \" + str(len(acc_100)/len(acc)*100) + \" %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macro Jaccard Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Model hat einen Macro Jaccard Score von: 67.04886367445796 %\n"
     ]
    }
   ],
   "source": [
    "Jaccard_score = []\n",
    "for p in range(len(real_all)):\n",
    "    jac = jaccard_score(real_all[p], results_all[p] , average=\"macro\")\n",
    "    Jaccard_score.append(jac)\n",
    "    \n",
    "print(\"Das Model hat einen Macro Jaccard Score von: \" + str(np.mean(Jaccard_score)*100)+ \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macro F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Model hat einen Macro F1 Score von: 71.28173552721996 %\n"
     ]
    }
   ],
   "source": [
    "F1_score = []\n",
    "for p in range(len(real_all)):\n",
    "    jac = f1_score(real_all[p], results_all[p] , average=\"macro\")\n",
    "    F1_score.append(jac)\n",
    "\n",
    "print(\"Das Model hat einen Macro F1 Score von: \" + str(np.mean(F1_score)*100)+ \" %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
