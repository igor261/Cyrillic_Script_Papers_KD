{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages & Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# read csv\n",
    "import pandas as pd\n",
    "import os\n",
    "# PdfMiner\n",
    "import glob\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "# Token Vectorization\n",
    "from langdetect import detect \n",
    "import fasttext.util\n",
    "import fasttext\n",
    "########################################\n",
    "from random import random\n",
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "########################################\n",
    "import regex as re  \n",
    "import string\n",
    "import re\n",
    "########################################\n",
    "from datetime import datetime\n",
    "import collections\n",
    "########################################\n",
    "# Keras imports for ML Model\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, LSTM, Bidirectional\n",
    "from keras.layers import Dropout, Flatten, GlobalAveragePooling2D, Embedding\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tei(tei_file):\n",
    "    with open(tei_file, 'r',encoding=\"utf-8\") as tei:\n",
    "        soup = BeautifulSoup(tei, 'lxml')\n",
    "        return soup\n",
    "    raise RuntimeError('Cannot generate a soup from the input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elem_to_text(elem, default=''):\n",
    "    if elem:\n",
    "        return elem.getText()\n",
    "    else:\n",
    "        return default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alan Turing authored many influential publications in computer science.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class Person:\n",
    "    firstname: str\n",
    "    middlename: str\n",
    "    surname: str\n",
    "\n",
    "turing_author = Person(firstname='Alan', middlename='M', surname='Turing')\n",
    "\n",
    "f\"{turing_author.firstname} {turing_author.surname} authored many influential publications in computer science.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEIFile(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.soup = read_tei(filename)\n",
    "        self._text = None\n",
    "        self._title = ''\n",
    "        self._abstract = ''\n",
    "\n",
    "    @property\n",
    "    def doi(self):\n",
    "        idno_elem = self.soup.find('idno', type='DOI')\n",
    "        if not idno_elem:\n",
    "            return ''\n",
    "        else:\n",
    "            return idno_elem.getText()\n",
    "\n",
    "    @property\n",
    "    def title(self):\n",
    "        if not self._title:\n",
    "            self._title = self.soup.title.getText()\n",
    "        return self._title\n",
    "\n",
    "    @property\n",
    "    def abstract(self):\n",
    "        if not self._abstract:\n",
    "            abstract = self.soup.abstract.getText(separator=' ', strip=True)\n",
    "            self._abstract = abstract\n",
    "        return self._abstract\n",
    "\n",
    "    @property\n",
    "    def authors(self):\n",
    "        authors_in_header = self.soup.analytic.find_all('author')\n",
    "\n",
    "        result = []\n",
    "        for author in authors_in_header:\n",
    "            persname = author.persname\n",
    "            if not persname:\n",
    "                continue\n",
    "            firstname = elem_to_text(persname.find(\"forename\", type=\"first\"))\n",
    "            middlename = elem_to_text(persname.find(\"forename\", type=\"middle\"))\n",
    "            surname = elem_to_text(persname.surname)\n",
    "            person = Person(firstname, middlename, surname)\n",
    "            result.append(person)\n",
    "        return result\n",
    "    \n",
    "    @property\n",
    "    def text(self):\n",
    "        if not self._text:\n",
    "            divs_text = []\n",
    "            for div in self.soup.body.find_all(\"div\"):\n",
    "                # div is neither an appendix nor references, just plain text.\n",
    "                if not div.get(\"type\"):\n",
    "                    div_text = div.get_text(separator=' ', strip=True)\n",
    "                    divs_text.append(div_text)\n",
    "\n",
    "            plain_text = \" \".join(divs_text)\n",
    "            self._text = plain_text\n",
    "        return self._text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that get the fullpath of files in a directory\n",
    "def listdir_fullpath(d):\n",
    "    return [os.path.join(d, f) for f in os.listdir(d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Funktion zum entfernen von Zeilenumbrüchen\n",
    "def removePassage(my_str):\n",
    "    my_str1 = re.sub(\"\\\\\\\\ud\", \" \", my_str)\n",
    "    my_str2 = re.sub(\"\\\\\\\\n\", \" \", my_str1)\n",
    "    return(my_str2)\n",
    "\n",
    "### Funktion zum parsen von PDF[nur erste Seite] zu String Format\n",
    "def extract_page_one(path):\n",
    "    output_string = StringIO()\n",
    "    \n",
    "    with open(path, 'rb') as in_file:\n",
    "        parser = PDFParser(in_file)\n",
    "        doc = PDFDocument(parser)\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        interpreter.process_page(list(PDFPage.create_pages(doc))[0])\n",
    "        return(output_string)\n",
    "    \n",
    "\n",
    "\n",
    "### Funktion zum Vergrößern der Input & Output Vektoren um NEWLINES\n",
    "def add_newlines(Tokens,Real_Tokens,y_final):\n",
    "    y_final_REAL = []\n",
    "    k = 0\n",
    "    m = 0\n",
    "    for i in range(len(Tokens)):\n",
    "        if k == 0:\n",
    "            j=i\n",
    "        else:\n",
    "            if m == 0:\n",
    "                j = k+1\n",
    "            else:\n",
    "                j = m+1\n",
    "        if Tokens[i] == Real_Tokens[j] : # Wenn Tokens gleich sind, dann übernehme y_final_REAL\n",
    "            y_final_REAL.append(y_final[i])\n",
    "            m = j\n",
    "        else:\n",
    "            for k in range(j,len(Real_Tokens)): # Sonst gehe die Real_Tokens durch bis match vorhanden\n",
    "\n",
    "                if Real_Tokens[k] == 'NEWLINE':\n",
    "                    y_final_REAL.append('Sonstiges')\n",
    "                \n",
    "#                 elif Real_Tokens[k] not in Tokens:\n",
    "#                     y_final_REAL.append('Sonstiges')\n",
    "\n",
    "                else:\n",
    "                    y_final_REAL.append(y_final[i])\n",
    "                    m=k\n",
    "                    break\n",
    "\n",
    "    RealTokens_final = Real_Tokens[:len(y_final_REAL)]\n",
    "    \n",
    "    index_title = [i for i, e in enumerate(y_final_REAL) if e == 'I-title']\n",
    "    if index_title==[]:\n",
    "        return(RealTokens_final,y_final_REAL)\n",
    "    else:\n",
    "        end_title = max(index_title)\n",
    "\n",
    "        ### lable NEWLINES im Titel als \"I-title\"        \n",
    "        for i in range(len(RealTokens_final)):\n",
    "            if RealTokens_final[i]=='NEWLINE':\n",
    "                if (y_final_REAL[i+1] =='I-title' or end_title>i) and y_final_REAL[i-1] in ('B-title','I-title'):\n",
    "                    y_final_REAL[i] = 'I-title'\n",
    "        ## Kann passieren, dass im Titel mehrere NEWLINES aufeinander folgen. Daher end_title>i um festzustellen, ob iwann später noch ein Titel Label kommt        \n",
    "\n",
    "        return(RealTokens_final,y_final_REAL)\n",
    "\n",
    "#Create Word Vector representation\n",
    "def detect_and_vectorize(tokens_sequence): #### input is the tokens list of ONE PAPER e.g. ergebnis_tokens[2]\n",
    "    \n",
    "    tokens_vectorized = []\n",
    "    lang = detect(' '.join(tokens_sequence))\n",
    "    \n",
    "    if (lang == 'ru'):\n",
    "        for i in range(len(tokens_sequence)):\n",
    "            tokens_vectorized.append(np.float16(ft_ru.get_word_vector(tokens_sequence[i])))\n",
    "                \n",
    "    elif (lang == 'bg'):\n",
    "        for i in range(len(tokens_sequence)):\n",
    "            tokens_vectorized.append(np.float16(ft_bg.get_word_vector(tokens_sequence[i])))\n",
    "                \n",
    "    else:  ## assume language == uk\n",
    "        for i in range(len(tokens_sequence)):\n",
    "            tokens_vectorized.append(np.float16(ft_uk.get_word_vector(tokens_sequence[i])))\n",
    "    \n",
    "    while len(tokens_vectorized)<1000:\n",
    "        tokens_vectorized.append(np.zeros(60))\n",
    "      \n",
    "    if len(tokens_vectorized)>1000:\n",
    "        del tokens_vectorized[1000:] \n",
    "\n",
    "    return np.array(tokens_vectorized)\n",
    "\n",
    "### Additional Features\n",
    "punctuations = '''!()[]{};:'\"\\<>/?@#$%^&*«»_–~.,-'''\n",
    "\n",
    "def compute_additional_features(tokens_sequence): #### input is the tokens list of ONE PAPER e.g. ergebnis_tokens[2]\n",
    "    \n",
    "    tokens = tokens_sequence\n",
    "    feature_upper = []\n",
    "    feature_capitalized = []\n",
    "    feature_autor_format = []\n",
    "    feature_punctation = []\n",
    "    feature_newline = []\n",
    "    feature_array = []\n",
    "    \n",
    "    while len(tokens)<1000:\n",
    "        tokens.append(str(0))\n",
    "    if len(tokens)>1000:\n",
    "        del tokens[1000:] \n",
    "    #print(tokens)    \n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] !='NEWLINE':\n",
    "            if str(tokens[i]).isupper():\n",
    "                feature_upper.append(1)\n",
    "\n",
    "            else:\n",
    "                feature_upper.append(0)\n",
    "        else: \n",
    "            feature_upper.append(0)\n",
    "\n",
    "        if tokens[i] !='NEWLINE':\n",
    "            if str(tokens[i][0]).isupper():\n",
    "                feature_capitalized.append(1)\n",
    "\n",
    "            else:\n",
    "                feature_capitalized.append(0)\n",
    "        else: \n",
    "            feature_capitalized.append(0)\n",
    "\n",
    "        if tokens[i] !='NEWLINE':\n",
    "            if re.match('.\\.',str(tokens[i])) != None and str(tokens[i]).isupper():\n",
    "                feature_autor_format.append(1)\n",
    "\n",
    "            else:\n",
    "                feature_autor_format.append(0)\n",
    "        else: \n",
    "            feature_autor_format.append(0)\n",
    "\n",
    "        if tokens[i] !='NEWLINE':\n",
    "            if any((c in punctuations) for c in str(tokens[i])):\n",
    "                feature_punctation.append(1)\n",
    "            else:\n",
    "                feature_punctation.append(0)\n",
    "        else: \n",
    "            feature_punctation.append(0)\n",
    "                \n",
    "        if tokens[i] =='NEWLINE':\n",
    "            feature_newline.append(1)\n",
    "        else: \n",
    "            feature_newline.append(0)\n",
    "    df = pd.DataFrame(list(zip(feature_upper, feature_capitalized,feature_autor_format ,feature_punctation,feature_newline)))  \n",
    "    feature_array = df.to_numpy(copy=True)\n",
    "    \n",
    "    return np.array(feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grobid: Entferne [[...]]\n",
    "### Funktion zum entfernen von 'Arrays' aus Autoren\n",
    "def removeAutor_grobid(my_str):\n",
    "    my_str1 = re.sub(\"\\[\\['\", \"\", my_str) \n",
    "    my_str2 = re.sub(\"'\\]\\]\", \"\", my_str1)\n",
    "    my_str3 = re.sub(\"'\", \"\", my_str2) \n",
    "    return(my_str3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Funktion zum entfernen von 'Arrays' aus Autoren\n",
    "def removeAutor(my_str):\n",
    "    my_str1 = re.sub(\"\\['\", \"\", my_str)\n",
    "    my_str2 = re.sub(\"'\\]\", \"\", my_str1)\n",
    "    my_str3 = re.sub(\"'\", \"\", my_str2)\n",
    "    return(my_str3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read TEI Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT RUN! Predictions of GROBID are already saved in 'grobid_16467.csv'\n",
    "### Transform the tei.xml files into a dataframe\n",
    "\n",
    "\n",
    "all_files = listdir_fullpath(\"D:\\_final_selection_16478\\GROBID_ALL\")\n",
    "# all_files = listdir_fullpath(\"D:\\_final_selection_16478\\out2\")\n",
    "\n",
    "Frame = pd.DataFrame({\"core_id\": [] , \"title\": [] , \"authors\": [] })\n",
    "i = 0\n",
    "for tei_doc in all_files:\n",
    "    tei = TEIFile(tei_doc)\n",
    "    core_id = re.sub(\".tei.xml\",\"\",re.sub(\"D:\\\\\\\\_final_selection_16478\\\\\\\\GROBID_ALL\\\\\\\\Core_ID_\",\"\",tei_doc))\n",
    "    authors = []\n",
    "    for i in range(len(tei.authors)):\n",
    "        if len(tei.authors[i].firstname)==1:\n",
    "            forename = tei.authors[i].firstname + \". \" + tei.authors[i].middlename + \".\"\n",
    "            surname = tei.authors[i].surname\n",
    "            name = [forename , surname]\n",
    "        elif len(tei.authors[i].middlename)==1:\n",
    "            forename = tei.authors[i].firstname +\" \" +tei.authors[i].middlename + \".\"\n",
    "            surname = tei.authors[i].surname\n",
    "            name = [forename , surname]\n",
    "        else:\n",
    "            forename = tei.authors[i].firstname + \" \" +tei.authors[i].middlename\n",
    "            surname = tei.authors[i].surname\n",
    "            name = [forename , surname]\n",
    "        authors.append(name)\n",
    "    \n",
    "    \n",
    "    Frame = Frame.append(pd.DataFrame(data = {\"core_id\": core_id , \"title\":tei.title , \"authors\":str(authors)},index = [i]), ignore_index=True)\n",
    "    i =+ 1\n",
    "\n",
    "# Frame.to_excel(\"grobid_0_8000.xlsx\")\n",
    "# Frame.to_excel(\"grobid_8000_16478.xlsx\")\n",
    "Frame.to_excel(\"grobid_16467.xlsx\")\n",
    "Frame.to_csv('grobid_16467.csv')\n",
    "Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate GROBID prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>core_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11083759</td>\n",
       "      <td>Середовище проведення освітніх вебінарів «WIP ...</td>\n",
       "      <td>[['Юрій ', 'Богачков']]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11083794</td>\n",
       "      <td>Комплексна підготовка дистанційних матеріалів ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11083797</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11083801</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11083807</td>\n",
       "      <td>Інструментальні засоби профорієнтації</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16462</th>\n",
       "      <td>95313001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16463</th>\n",
       "      <td>95313006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16464</th>\n",
       "      <td>95313010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16465</th>\n",
       "      <td>95313012</td>\n",
       "      <td>ТЕХНОЛОГІЯ ПОКВАРТИРНОГО ОБЛІКУ ТЕПЛОВОЇ ЕНЕРГІЇ</td>\n",
       "      <td>[['Горбачьова А О', '']]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16466</th>\n",
       "      <td>95313017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16467 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        core_id                                              title  \\\n",
       "0      11083759  Середовище проведення освітніх вебінарів «WIP ...   \n",
       "1      11083794  Комплексна підготовка дистанційних матеріалів ...   \n",
       "2      11083797                                                NaN   \n",
       "3      11083801                                                NaN   \n",
       "4      11083807              Інструментальні засоби профорієнтації   \n",
       "...         ...                                                ...   \n",
       "16462  95313001                                                NaN   \n",
       "16463  95313006                                                NaN   \n",
       "16464  95313010                                                NaN   \n",
       "16465  95313012   ТЕХНОЛОГІЯ ПОКВАРТИРНОГО ОБЛІКУ ТЕПЛОВОЇ ЕНЕРГІЇ   \n",
       "16466  95313017                                                NaN   \n",
       "\n",
       "                        authors  \n",
       "0       [['Юрій ', 'Богачков']]  \n",
       "1                            []  \n",
       "2                            []  \n",
       "3                            []  \n",
       "4                            []  \n",
       "...                         ...  \n",
       "16462                        []  \n",
       "16463                        []  \n",
       "16464                        []  \n",
       "16465  [['Горбачьова А О', '']]  \n",
       "16466                        []  \n",
       "\n",
       "[16467 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta = pd.read_csv('grobid_16467.csv', sep = ',')#  , encoding= 'utf-16')\n",
    "df_meta.drop('Unnamed: 0' , axis = 1 , inplace=True)\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_15553 = \"D:/_final_selection_16478/PDFs_15553/\"\n",
    "\n",
    "\n",
    "pdf = os.listdir(path_15553)\n",
    "\n",
    "\n",
    "files_core_id = []\n",
    "files_paths = []\n",
    "for elem in pdf:\n",
    "    core = int(re.sub(\".pdf\",\"\",re.sub(\"Core_ID_\",\"\",elem)))\n",
    "    if core in list(df_meta.core_id):\n",
    "        files_core_id.append(core)\n",
    "#         files_paths.append(\"D:/_final_selection_16478/all_pdf/\" + elem)\n",
    "        files_paths.append(path_15553 + elem)\n",
    "print(len(files_paths))\n",
    "print(files_paths[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = df_meta[df_meta.core_id.isin(files_core_id)].reset_index()\n",
    "df_meta.drop('index' , axis = 1 , inplace=True)\n",
    "print(df_meta.shape)\n",
    "print(\"{} Titel und {} Autoren sind NA\".format(sum(df_meta.title.isna()),sum(df_meta.authors == \"[]\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0%\n",
      "3.2150205761316872%\n",
      "6.4300411522633745%\n",
      "9.645061728395062%\n",
      "12.860082304526749%\n",
      "16.075102880658438%\n",
      "19.290123456790123%\n",
      "22.505144032921812%\n",
      "25.720164609053498%\n",
      "28.935185185185187%\n",
      "32.150205761316876%\n",
      "35.365226337448554%\n",
      "38.58024691358025%\n",
      "41.79526748971193%\n",
      "45.010288065843625%\n",
      "48.2253086419753%\n",
      "51.440329218106996%\n",
      "54.65534979423868%\n",
      "57.870370370370374%\n",
      "61.08539094650206%\n",
      "64.30041152263375%\n",
      "67.51543209876543%\n",
      "70.73045267489711%\n",
      "73.9454732510288%\n",
      "77.1604938271605%\n",
      "80.37551440329219%\n",
      "83.59053497942386%\n",
      "86.80555555555556%\n",
      "90.02057613168725%\n",
      "93.23559670781893%\n",
      "96.4506172839506%\n",
      "99.6656378600823%\n"
     ]
    }
   ],
   "source": [
    "### extract text \n",
    "\n",
    "all_pdf_text = [] \n",
    "start_time = datetime.now()\n",
    "for i in range(len(files_paths)):\n",
    "    try:\n",
    "        all_pdf_text.append(extract_page_one(files_paths[i]).getvalue())\n",
    "        if i % 500 == 0:\n",
    "            print(str((i/len(files_paths))*100)+'%')\n",
    "    except:\n",
    "        all_pdf_text.append(\"Einlesen nicht moeglich\")\n",
    "        print(\"FEHLER\")\n",
    "    \n",
    "end_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15552"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Import REAL Meta Data\n",
    "path_meta = 'D:/_final_selection_16478/'\n",
    "\n",
    "df_meta_real = pd.read_csv(path_meta + 'final_items_15553.csv', sep = ',')#  , encoding= 'utf-16')\n",
    "# df_meta.drop('Unnamed: 0' , axis = 1 , inplace=True)\n",
    "# df_meta_real\n",
    "\n",
    "### get all titles from meta data with core_ids of fulltext\n",
    "titles_real = []\n",
    "for i in range(len(files_core_id)):\n",
    "    index = df_meta_real.index[df_meta_real['coreId'] == int(files_core_id[i])].tolist()\n",
    "    if index == []:\n",
    "        titles_real.append('Keine Meta Daten gefunden')\n",
    "    else: \n",
    "        index = index[0]\n",
    "        title_pdf  = df_meta_real.loc[index,'title']\n",
    "        titles_real.append(title_pdf)\n",
    "len(titles_real)\n",
    "\n",
    "### Get autor for the PDF´s\n",
    "######## PROBLEM: verschiedene Schreibweisen Meta <-> PDF\n",
    "autors_real = []\n",
    "for i in range(len(files_core_id)):\n",
    "    index = df_meta_real.index[df_meta_real['coreId'] == int(files_core_id[i])].tolist()\n",
    "    index = index[0]\n",
    "    autor_pdf  = df_meta_real.loc[index,'authors']\n",
    "\n",
    "    autor_pdf = removeAutor(autor_pdf).split(\",\")\n",
    "    for j in range(len(autor_pdf)):\n",
    "        autor_pdf[j] = ' '.join(autor_pdf[j].split()) ## Entferne überflüssige Whitespaces (auch am Anfang)\n",
    "        \n",
    "    autors_real.append(autor_pdf)\n",
    "len(autors_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Label vectors for the GROBID Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15552"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get all titles from meta data with core_ids of fulltext\n",
    "titles = []\n",
    "for i in range(len(df_meta.core_id)):\n",
    "    index = df_meta.index[df_meta['core_id'] == int(df_meta.core_id[i])].tolist()\n",
    "    if df_meta.title.isna()[i]:\n",
    "        titles.append('Keine Meta Daten gefunden')\n",
    "    else: \n",
    "        index = index[0]\n",
    "        title_pdf  = df_meta.loc[index,'title']\n",
    "        titles.append(title_pdf)\n",
    "\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15552"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get autor for the PDF´s\n",
    "######## PROBLEM: verschiedene Schreibweisen Meta <-> PDF\n",
    "autors = []\n",
    "for i in range(len(df_meta.core_id)):\n",
    "    index = df_meta.index[df_meta['core_id'] == int(df_meta.core_id[i])].tolist()\n",
    "    index = index[0]\n",
    "    autor_pdf  = df_meta.loc[index,'authors']\n",
    "\n",
    "    autor_pdf = removeAutor_grobid(autor_pdf).split(\",\")\n",
    "    for j in range(len(autor_pdf)):\n",
    "        autor_pdf[j] = ' '.join(autor_pdf[j].split()) ## Entferne überflüssige Whitespaces (auch am Anfang)\n",
    "        \n",
    "    autors.append(autor_pdf)\n",
    "len(autors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Funktion, die die Daten labelt \n",
    "\n",
    "kein_autor = []\n",
    "kein_titel = []\n",
    "error_papers = []\n",
    "ergebnis_tokens = []\n",
    "ergebnis_label = []\n",
    "### Real Meta\n",
    "ergebnis_label_real = []\n",
    "###\n",
    "\n",
    "anzahl_papers = len(df_meta.core_id)\n",
    "\n",
    "for paper in range(anzahl_papers):\n",
    "\n",
    "    title = ' '.join(removePassage(titles[paper]).split()).lower() # Remove excces Whitespace & to lowercase\n",
    "    title = re.sub(\"\\(\",\"\\(\",title) # () as non-regex string\n",
    "    title = re.sub(\"\\)\",\"\\)\",title)\n",
    "    title = re.sub(\"\\*\",\"\\*\",title) # * as non-regex string\n",
    "\n",
    "    title_index = re.search(title, ' '.join(all_pdf_text[paper].split()).lower()) # search for the title\n",
    "    \n",
    "    #### Real Meta\n",
    "    title_real = ' '.join(removePassage(titles_real[paper]).split()).lower() # Remove excces Whitespace & to lowercase\n",
    "    title_real = re.sub(\"\\(\",\"\\(\",title_real) # () as non-regex string\n",
    "    title_real = re.sub(\"\\)\",\"\\)\",title_real)\n",
    "    title_real = re.sub(\"\\*\",\"\\*\",title_real) # * as non-regex string\n",
    "\n",
    "    title_index_real = re.search(title_real, ' '.join(all_pdf_text[paper].split()).lower()) # search for the title\n",
    "    ######\n",
    "    \n",
    "#     print('CoreID:  ' + str(files_core_id[paper]))\n",
    "\n",
    "    if title_index==None:\n",
    "        Text_pdf_0 = ' '.join(all_pdf_text[paper].split())\n",
    "        \n",
    "        kein_titel.append(df_meta.core_id[paper])\n",
    "        y_final= np.repeat('Sonstiges',len(Text_pdf_0.split()))\n",
    "    else:\n",
    "        \n",
    "        Text_pdf_0 = ' '.join(all_pdf_text[paper].split())\n",
    "\n",
    "        ##### TITLE ################################################\n",
    "        if title_index.start()==0:\n",
    "            teil_B = \"\"\n",
    "        else:\n",
    "            teil_B = Text_pdf_0[0:title_index.start()-1]\n",
    "        teil_T = Text_pdf_0[title_index.start():title_index.end()]\n",
    "        teil_E = Text_pdf_0[title_index.end()+1:len(Text_pdf_0)]\n",
    "\n",
    "        y_teil1 = np.repeat('Sonstiges',len(teil_B.split()))\n",
    "        y_teil2 = np.append(['B-title'],np.repeat('I-title',len(teil_T.split())-1))\n",
    "        y_teil3 = np.repeat('Sonstiges',len(teil_E.split()))\n",
    "\n",
    "        y_final = np.concatenate((y_teil1, y_teil2 , y_teil3), axis=None)\n",
    "        \n",
    "    ### Real Meta    \n",
    "    if title_index_real!=None:\n",
    "        Text_pdf_0 = ' '.join(all_pdf_text[paper].split())\n",
    "\n",
    "        ##### TITLE ################################################\n",
    "        if title_index_real.start()==0:\n",
    "            teil_B_real = \"\"\n",
    "        else:\n",
    "            teil_B_real = Text_pdf_0[0:title_index_real.start()-1]\n",
    "        teil_T_real = Text_pdf_0[title_index_real.start():title_index_real.end()]\n",
    "        teil_E_real = Text_pdf_0[title_index_real.end()+1:len(Text_pdf_0)]\n",
    "\n",
    "        y_teil1_r = np.repeat('Sonstiges',len(teil_B_real.split()))\n",
    "        y_teil2_r = np.append(['B-title'],np.repeat('I-title',len(teil_T_real.split())-1))\n",
    "        y_teil3_r = np.repeat('Sonstiges',len(teil_E_real.split()))\n",
    "\n",
    "        y_final_real = np.concatenate((y_teil1_r, y_teil2_r , y_teil3_r), axis=None)\n",
    "    ###\n",
    "    \n",
    "    ##### Get Text\n",
    "    all_pdf_text1 = re.sub(\"\\\\n\",\" NEWLINE \",all_pdf_text[paper])\n",
    "    Text_pdf_0_NL = ' '.join(all_pdf_text1.split())\n",
    "\n",
    "    Tokens = Text_pdf_0.split()\n",
    "    Labels = y_final\n",
    "    Real_Tokens = Text_pdf_0_NL.split()\n",
    "\n",
    "    Tokens = all_pdf_text[paper].split()\n",
    "\n",
    "    Tokens_final_lower = []\n",
    "    for i in range(len(Tokens)):\n",
    "        Tokens_final_lower.append(Tokens[i].lower())\n",
    "    try:\n",
    "        if autors[paper]!= ['[]']:\n",
    "\n",
    "            autors_surname = []\n",
    "            for i in range(len(autors[paper])):\n",
    "                if i % 2 == 0:\n",
    "                    autors_surname.append(autors[paper][i])\n",
    "\n",
    "            autors_surname_lower = []\n",
    "            for i in range(len(autors_surname)):\n",
    "                autors_surname_lower.append(autors_surname[i].lower())\n",
    "\n",
    "            if re.match('.\\.',autors[paper][1]) == None:\n",
    "                autors_forename = []\n",
    "                for i in range(len(autors[paper])):\n",
    "                    if i % 2 == 1:\n",
    "                        autors_forename.append(autors[paper][i].split())\n",
    "\n",
    "                autors_forename = list(np.concatenate((autors_forename), axis=None))\n",
    "                autors_forename_lower = []\n",
    "                for i in range(len(autors_forename)):\n",
    "                    autors_forename_lower.append(autors_forename[i].lower())\n",
    "\n",
    "                autors_surname_lower = list(np.concatenate((autors_forename_lower,autors_surname_lower), axis=None))\n",
    "\n",
    "\n",
    "            vec_autor = []\n",
    "            for token in Tokens_final_lower:\n",
    "                line = any(word in token for word in autors_surname_lower)\n",
    "                vec_autor.append(line)\n",
    "\n",
    "            index_autor = [i for i, e in enumerate(vec_autor) if e == True]\n",
    "\n",
    "            if title_index!=None:\n",
    "                if len(index_autor)>(len(autors_surname_lower)):\n",
    "                    diff = len(index_autor) - len(autors_surname_lower)\n",
    "                    dist = []\n",
    "                    for j in range(len(index_autor)):\n",
    "                        dist.append(abs(index_autor[j]-np.where(y_final==\"B-title\")[0][0]))\n",
    "\n",
    "                    dict1 = dict(zip(dist , index_autor))\n",
    "\n",
    "                    dist.sort(reverse = True)\n",
    "\n",
    "                    for k in range(len(dist[0:diff])):\n",
    "                        vec_autor[dict1[dist[0:diff][k]]] = False\n",
    "\n",
    "            for i in range(len(y_final)):\n",
    "                if vec_autor[i] == True:\n",
    "                    y_final[i] = 'autor'\n",
    "\n",
    "            if True not in vec_autor:\n",
    "                kein_autor.append(files_core_id[paper])\n",
    "\n",
    "            if re.match('.\\.',autors[paper][1]) != None:\n",
    "\n",
    "                index_autor_true = [i for i, e in enumerate(vec_autor) if e == True]\n",
    "\n",
    "                for w in range(len(index_autor_true)):\n",
    "                    index = index_autor_true[w]\n",
    "                    for t in range(index - 4,index + 4):\n",
    "                        if re.match('.\\.',Tokens_final_lower[t]) != None and Tokens[t].isupper():\n",
    "                            y_final[t] = 'autor'\n",
    "        ### Real Meta\n",
    "        autors_surname_real = []\n",
    "        for i in range(len(autors_real[paper])):\n",
    "            if i % 2 == 0:\n",
    "                autors_surname_real.append(autors_real[paper][i])\n",
    "\n",
    "        autors_surname_lower_real = []\n",
    "        for i in range(len(autors_surname_real)):\n",
    "            autors_surname_lower_real.append(autors_surname_real[i].lower())\n",
    "\n",
    "        if re.match('.\\.',autors_real[paper][1]) == None:\n",
    "            autors_forename_real = []\n",
    "            for i in range(len(autors_real[paper])):\n",
    "                if i % 2 == 1:\n",
    "                    autors_forename_real.append(autors_real[paper][i].split())\n",
    "\n",
    "            autors_forename_real = list(np.concatenate((autors_forename_real), axis=None))\n",
    "            autors_forename_lower_real = []\n",
    "            for i in range(len(autors_forename_real)):\n",
    "                autors_forename_lower_real.append(autors_forename_real[i].lower())\n",
    "\n",
    "            autors_surname_lower_real = list(np.concatenate((autors_forename_lower_real,autors_surname_lower_real), axis=None))\n",
    "\n",
    "\n",
    "        vec_autor_real = []\n",
    "        for token in Tokens_final_lower:\n",
    "            line_real = any(word in token for word in autors_surname_lower_real)\n",
    "            vec_autor_real.append(line_real)\n",
    "\n",
    "        index_autor_real = [i for i, e in enumerate(vec_autor_real) if e == True]\n",
    "\n",
    "        if title_index_real!=None:\n",
    "            if len(index_autor_real)>(len(autors_surname_lower_real)):\n",
    "                diff = len(index_autor_real) - len(autors_surname_lower_real)\n",
    "                dist = []\n",
    "                for j in range(len(index_autor_real)):\n",
    "                    dist.append(abs(index_autor_real[j]-np.where(y_final_real==\"B-title\")[0][0]))\n",
    "\n",
    "                dict1 = dict(zip(dist , index_autor_real))\n",
    "\n",
    "                dist.sort(reverse = True)\n",
    "\n",
    "                for k in range(len(dist[0:diff])):\n",
    "                    vec_autor_real[dict1[dist[0:diff][k]]] = False\n",
    "\n",
    "        for i in range(len(y_final_real)):\n",
    "            if vec_autor_real[i] == True:\n",
    "                y_final_real[i] = 'autor'\n",
    "\n",
    "#         if True not in vec_autor_real:\n",
    "#             kein_autor.append(files_core_id[paper])\n",
    "\n",
    "        if re.match('.\\.',autors_real[paper][1]) != None:\n",
    "\n",
    "            index_autor_true_real = [i for i, e in enumerate(vec_autor_real) if e == True]\n",
    "\n",
    "            for w in range(len(index_autor_true_real)):\n",
    "                index = index_autor_true_real[w]\n",
    "                for t in range(index - 4,index + 4):\n",
    "                    if re.match('.\\.',Tokens_final_lower[t]) != None and Tokens[t].isupper():\n",
    "                        y_final_real[t] = 'autor'\n",
    "        ###\n",
    "\n",
    "        RealTokens_final = add_newlines(Tokens,Real_Tokens,y_final)[0]\n",
    "        y_final_REAL = add_newlines(Tokens,Real_Tokens,y_final)[1]\n",
    "        ### Real Meta\n",
    "        y_final_REAL2 = add_newlines(Tokens,Real_Tokens,y_final_real)[1]\n",
    "        ergebnis_label_real.append(y_final_REAL2)\n",
    "        ###\n",
    "        ergebnis_label.append(y_final_REAL)\n",
    "        ergebnis_tokens.append(RealTokens_final)\n",
    "    except:\n",
    "        error_papers.append(files_core_id[paper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "13321\n",
      "2\n",
      "15550\n",
      "15550\n",
      "15550\n"
     ]
    }
   ],
   "source": [
    "print(len(kein_autor))\n",
    "print(len(kein_titel))\n",
    "print(len(error_papers))\n",
    "\n",
    "print(len(ergebnis_label))\n",
    "print(len(ergebnis_tokens))\n",
    "print(len(ergebnis_label_real))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.55426828645383 %\n",
      "Completly correct classifications: 0.13504823151125403 %\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "acc_100 = []\n",
    "for i in range(len(ergebnis_tokens)):\n",
    "    acc.append(accuracy_score(ergebnis_label[i], ergebnis_label_real[i]))\n",
    "    if ergebnis_label[i]== ergebnis_label_real[i]:\n",
    "        acc_100.append(1)\n",
    "        \n",
    "print(\"Accuracy: \" + str(np.mean(acc)*100) + \" %\")\n",
    "print(\"Completly correct classifications: \" + str(len(acc_100)/len(acc)*100) + \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macro Jaccard Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Jaccard Score: 27.60339278754594 %\n"
     ]
    }
   ],
   "source": [
    "Jaccard_score = []\n",
    "for p in range(len(ergebnis_label_real)):\n",
    "    jac = jaccard_score(ergebnis_label_real[p], ergebnis_label[p] , average=\"macro\")\n",
    "    Jaccard_score.append(jac)\n",
    "    \n",
    "print(\"Macro Jaccard Score: \" + str(np.mean(Jaccard_score)*100) + \" %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macro F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 Score: 28.127084463729908 %\n"
     ]
    }
   ],
   "source": [
    "F1_score = []\n",
    "for p in range(len(ergebnis_label_real)):\n",
    "    f1 = f1_score(ergebnis_label_real[p], ergebnis_label[p] , average=\"macro\")\n",
    "    F1_score.append(f1)\n",
    "    \n",
    "print(\"Macro F1 Score: \" + str(np.mean(F1_score)*100) + \" %\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
